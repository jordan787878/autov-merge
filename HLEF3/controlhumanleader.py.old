import numpy as np
import copy
from controlegofollower import ControlEgoFollow
from car2 import CarVer2
from trajectory2 import *
from visual2 import *

##############################
# For Merging Behavior Reward
GOAL_X = 200
# For Collision Detect
MERGE_SAFE_DIST = 10 
##############################


class ControlHumanLead:
    def __init__(self,car_my,car_op, name):
        print("init human leader controller")
        self.name = name

        # Link My Car
        self.car_my = car_my

        # Absolute Ego Car
        self.car_ego_abs = car_op

        # Link Op Car
        # Create ego follower car
        car_ego = CarVer2(s = car_op.s,
                        actionspace = car_op.actionspace,
                        ID = "ego_f",
                        horizon = car_op.horizon,
                        dynamics = car_op.dynamics
                        )
        # Create ego follower controller
        cont_EF = ControlEgoFollow(car_ego,self.car_my,"ego follower")
        # Link ego follower car/controller
        car_ego.add_controller(cont_EF)
        # Link opp car to car_ego
        self.car_op = car_ego
        self.car_op.set_lane_width(car_op.lane_width)

        # Check
        print("human lead control check link")
        print("human lead ego controller state:\t",self.car_op.s)

        ########################################################

        # Obtain horizon
        self.horizon = self.car_my.horizon

        # Init action
        self.action = None

        # Get action set
        self.act_set_my = self.car_my.get_all_actions()
        self.act_set_op = self.car_op.get_all_ego_actions()

        # Prepare num actions
        self.num_act_my = self.act_set_my.shape[0]
        self.num_act_op = self.act_set_op.shape[0]

        # Discount Vector
        self.discount = np.zeros(self.horizon)
        for i in range(self.horizon):
            self.discount[i] = pow(0.9,i)

    ##################################################
    def select_action(self):
#        print("\n----- human lead select action() -----")
#        print("observe ego state")
        self.car_op.s = copy.deepcopy(self.car_ego_abs.s)
#        print("estimate ego state:\t",self.car_op.s)
        #print("absolute ego state:\t",self.car_ego_abs.s)

        acts_opt, traj_opt = self.select_opt_actions(False, None)

#        print("human lead opt actions:\t\t\t\t",acts_opt)

        self.action = acts_opt[0]

        # random policy
        #self.action = np.random.randint(3)

        return self.action

    ###################################################
    def select_opt_actions(self, call_from_ego, actual_state):
        if call_from_ego: # Flag of Ego Merge Controller Call
#            print("ego merge -> human lead.select_opt_actions()")
            # update ego state
            self.car_op.s = actual_state
            #print("actual state:\t",actual_state)
#            print("human lead: estimate ego state:\t",self.car_op.s)

        # Init Qli Matrix
        Qli = np.zeros(self.num_act_my)

        # Get optimal ego follower traj
        acts_ego_opt, traj_ego_opt = self.car_op.controller.select_opt_actions(True,None)

#        print("human lead estimate ego folo opt actions:\t",acts_ego_opt)

        # Get human trajectory
        traj_hum = get_hum_traj(self.car_my.s,
                                self.act_set_my,
                                self.car_my.horizon,
                                self.car_my.dynamics)

        for i in range(self.num_act_my):
            traj_hum0 = traj_hum[i,:]
            Qli[i] = self.compute_reward(traj_ego_opt,traj_hum0)

#        if call_from_ego == False:
#            print("human lead think opt. ego follower action: ",acts_ego_opt)
#            vis_Qmatrix(Qli,self.act_set_my, np.array([1]), 'human lead Qli')

        # Max Ql 
        traj_opt_idx = np.argmax(Qli)
        traj_opt = traj_hum[traj_opt_idx,:,:]
        acts_opt = self.act_set_my[traj_opt_idx,:]

        return acts_opt, traj_opt

    #######################################################
    def compute_reward(self,traj_ego,traj_hum):

        s_pred = np.hstack((traj_ego.T,traj_hum.T))

        ##### Highway Leader #####
        # Init
        R_distance = np.zeros(s_pred.shape[0])
        R_collision = -100
        R = 0

        ###### Collision Reward ##### (entire horizon)
        R_collision = self.if_collision(s_pred)*R_collision

        ##### Distance Reward #####
        goal = GOAL_X

        # Normalized Distance Reward for each step, (min: -1, max: 0)
        R_distance = -abs(s_pred[:,4] - goal)/(goal)

        # Reward
        R = R_distance + R_collision
        #R = np.minimum(R_distance, R_collision)

        # Discount factor for each step
        R = R*self.discount

        # Cumulative reward over entire horizon
        R = R.sum()

        return R

    #######################################################
    def if_collision(self,s_pred):
        col_flag = np.zeros(self.horizon)

        x_ego = s_pred[:,0]
        y_ego = s_pred[:,1]
        x_other = s_pred[:,4]
        y_other = s_pred[:,5]

        x_diff = (x_ego-x_other)
        y_diff = (y_ego-y_other)

        for k in range(self.horizon):
            if abs(x_diff[k]) >= MERGE_SAFE_DIST or abs(y_diff[k]) > 2.0:
                col_flag[k] = 0
            else:
                col_flag[k] = 1

        return col_flag





